{"ast":null,"code":"'use strict';\n\nvar BB = require('bluebird');\n\nvar contentPath = require('./content/path');\n\nvar figgyPudding = require('figgy-pudding');\n\nvar finished = BB.promisify(require('mississippi').finished);\n\nvar fixOwner = require('./util/fix-owner');\n\nvar fs = require('graceful-fs');\n\nvar glob = BB.promisify(require('glob'));\n\nvar index = require('./entry-index');\n\nvar path = require('path');\n\nvar rimraf = BB.promisify(require('rimraf'));\n\nvar ssri = require('ssri');\n\nBB.promisifyAll(fs);\nvar VerifyOpts = figgyPudding({\n  concurrency: {\n    \"default\": 20\n  },\n  filter: {},\n  log: {\n    \"default\": {\n      silly: function silly() {}\n    }\n  }\n});\nmodule.exports = verify;\n\nfunction verify(cache, opts) {\n  opts = VerifyOpts(opts);\n  opts.log.silly('verify', 'verifying cache at', cache);\n  return BB.reduce([markStartTime, fixPerms, garbageCollect, rebuildIndex, cleanTmp, writeVerifile, markEndTime], function (stats, step, i) {\n    var label = step.name || \"step #\".concat(i);\n    var start = new Date();\n    return BB.resolve(step(cache, opts)).then(function (s) {\n      s && Object.keys(s).forEach(function (k) {\n        stats[k] = s[k];\n      });\n      var end = new Date();\n\n      if (!stats.runTime) {\n        stats.runTime = {};\n      }\n\n      stats.runTime[label] = end - start;\n      return stats;\n    });\n  }, {}).tap(function (stats) {\n    stats.runTime.total = stats.endTime - stats.startTime;\n    opts.log.silly('verify', 'verification finished for', cache, 'in', \"\".concat(stats.runTime.total, \"ms\"));\n  });\n}\n\nfunction markStartTime(cache, opts) {\n  return {\n    startTime: new Date()\n  };\n}\n\nfunction markEndTime(cache, opts) {\n  return {\n    endTime: new Date()\n  };\n}\n\nfunction fixPerms(cache, opts) {\n  opts.log.silly('verify', 'fixing cache permissions');\n  return fixOwner.mkdirfix(cache, cache).then(function () {\n    // TODO - fix file permissions too\n    return fixOwner.chownr(cache, cache);\n  }).then(function () {\n    return null;\n  });\n} // Implements a naive mark-and-sweep tracing garbage collector.\n//\n// The algorithm is basically as follows:\n// 1. Read (and filter) all index entries (\"pointers\")\n// 2. Mark each integrity value as \"live\"\n// 3. Read entire filesystem tree in `content-vX/` dir\n// 4. If content is live, verify its checksum and delete it if it fails\n// 5. If content is not marked as live, rimraf it.\n//\n\n\nfunction garbageCollect(cache, opts) {\n  opts.log.silly('verify', 'garbage collecting content');\n  var indexStream = index.lsStream(cache);\n  var liveContent = new Set();\n  indexStream.on('data', function (entry) {\n    if (opts.filter && !opts.filter(entry)) {\n      return;\n    }\n\n    liveContent.add(entry.integrity.toString());\n  });\n  return finished(indexStream).then(function () {\n    var contentDir = contentPath._contentDir(cache);\n\n    return glob(path.join(contentDir, '**'), {\n      follow: false,\n      nodir: true,\n      nosort: true\n    }).then(function (files) {\n      return BB.resolve({\n        verifiedContent: 0,\n        reclaimedCount: 0,\n        reclaimedSize: 0,\n        badContentCount: 0,\n        keptSize: 0\n      }).tap(function (stats) {\n        return BB.map(files, function (f) {\n          var split = f.split(/[/\\\\]/);\n          var digest = split.slice(split.length - 3).join('');\n          var algo = split[split.length - 4];\n          var integrity = ssri.fromHex(digest, algo);\n\n          if (liveContent.has(integrity.toString())) {\n            return verifyContent(f, integrity).then(function (info) {\n              if (!info.valid) {\n                stats.reclaimedCount++;\n                stats.badContentCount++;\n                stats.reclaimedSize += info.size;\n              } else {\n                stats.verifiedContent++;\n                stats.keptSize += info.size;\n              }\n\n              return stats;\n            });\n          } else {\n            // No entries refer to this content. We can delete.\n            stats.reclaimedCount++;\n            return fs.statAsync(f).then(function (s) {\n              return rimraf(f).then(function () {\n                stats.reclaimedSize += s.size;\n                return stats;\n              });\n            });\n          }\n        }, {\n          concurrency: opts.concurrency\n        });\n      });\n    });\n  });\n}\n\nfunction verifyContent(filepath, sri) {\n  return fs.statAsync(filepath).then(function (stat) {\n    var contentInfo = {\n      size: stat.size,\n      valid: true\n    };\n    return ssri.checkStream(fs.createReadStream(filepath), sri)[\"catch\"](function (err) {\n      if (err.code !== 'EINTEGRITY') {\n        throw err;\n      }\n\n      return rimraf(filepath).then(function () {\n        contentInfo.valid = false;\n      });\n    }).then(function () {\n      return contentInfo;\n    });\n  })[\"catch\"]({\n    code: 'ENOENT'\n  }, function () {\n    return {\n      size: 0,\n      valid: false\n    };\n  });\n}\n\nfunction rebuildIndex(cache, opts) {\n  opts.log.silly('verify', 'rebuilding index');\n  return index.ls(cache).then(function (entries) {\n    var stats = {\n      missingContent: 0,\n      rejectedEntries: 0,\n      totalEntries: 0\n    };\n    var buckets = {};\n\n    for (var k in entries) {\n      if (entries.hasOwnProperty(k)) {\n        var hashed = index._hashKey(k);\n\n        var entry = entries[k];\n        var excluded = opts.filter && !opts.filter(entry);\n        excluded && stats.rejectedEntries++;\n\n        if (buckets[hashed] && !excluded) {\n          buckets[hashed].push(entry);\n        } else if (buckets[hashed] && excluded) {// skip\n        } else if (excluded) {\n          buckets[hashed] = [];\n          buckets[hashed]._path = index._bucketPath(cache, k);\n        } else {\n          buckets[hashed] = [entry];\n          buckets[hashed]._path = index._bucketPath(cache, k);\n        }\n      }\n    }\n\n    return BB.map(Object.keys(buckets), function (key) {\n      return rebuildBucket(cache, buckets[key], stats, opts);\n    }, {\n      concurrency: opts.concurrency\n    }).then(function () {\n      return stats;\n    });\n  });\n}\n\nfunction rebuildBucket(cache, bucket, stats, opts) {\n  return fs.truncateAsync(bucket._path).then(function () {\n    // This needs to be serialized because cacache explicitly\n    // lets very racy bucket conflicts clobber each other.\n    return BB.mapSeries(bucket, function (entry) {\n      var content = contentPath(cache, entry.integrity);\n      return fs.statAsync(content).then(function () {\n        return index.insert(cache, entry.key, entry.integrity, {\n          metadata: entry.metadata,\n          size: entry.size\n        }).then(function () {\n          stats.totalEntries++;\n        });\n      })[\"catch\"]({\n        code: 'ENOENT'\n      }, function () {\n        stats.rejectedEntries++;\n        stats.missingContent++;\n      });\n    });\n  });\n}\n\nfunction cleanTmp(cache, opts) {\n  opts.log.silly('verify', 'cleaning tmp directory');\n  return rimraf(path.join(cache, 'tmp'));\n}\n\nfunction writeVerifile(cache, opts) {\n  var verifile = path.join(cache, '_lastverified');\n  opts.log.silly('verify', 'writing verifile to ' + verifile);\n\n  try {\n    return fs.writeFileAsync(verifile, '' + +new Date());\n  } finally {\n    fixOwner.chownr.sync(cache, verifile);\n  }\n}\n\nmodule.exports.lastRun = lastRun;\n\nfunction lastRun(cache) {\n  return fs.readFileAsync(path.join(cache, '_lastverified'), 'utf8').then(function (data) {\n    return new Date(+data);\n  });\n}","map":null,"metadata":{},"sourceType":"module"}